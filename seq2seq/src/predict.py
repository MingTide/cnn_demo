import torch
from tokenizer import ChineseTokenizer, EnglishTokenizer
from model import TranslationEncoder, TranslationDecoder
import config

def predict_batch(input_tensor, encoder, decoder, en_tokenizer, device):
  """
  对一个 batch 的输入进行翻译预测。

  :param input_tensor: 中文输入张量，形状 (batch_size, seq_len)。
  :param encoder: 编码器。
  :param decoder: 解码器。
  :param en_tokenizer: 英文分词器。
  :param device: 设备。
  :return: 英文 token 索引列表。
  """
  encoder.eval()
  decoder.eval()

  with torch.no_grad():
    # 编码器前向传播
    encoder_output, encoder_hidden = encoder(input_tensor)

    # 拼接双向 GRU 的最后隐藏状态作为上下文向量
    context_vector = torch.cat([encoder_hidden[-2], encoder_hidden[-1]], dim=1)

    batch_size = input_tensor.shape[0]
    decoder_input = torch.full(
      size=(batch_size, 1),
      fill_value=en_tokenizer.sos_token_index,
      device=device
    )
    decoder_hidden = context_vector.unsqueeze(0)

    generated = [[] for _ in range(batch_size)]
    finished = [False for _ in range(batch_size)]

    for step in range(1, config.SEQ_LEN):
      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
      predict_indexes = decoder_output.argmax(dim=-1)

      for i in range(batch_size):
        if finished[i]:
          continue
        token_id = predict_indexes[i].item()
        if token_id == en_tokenizer.eos_token_index:
          finished[i] = True
          continue
        generated[i].append(token_id)

      if all(finished):
        break

      decoder_input = predict_indexes

    return generated

def predict(zh_sentence, encoder, decoder, zh_tokenizer, en_tokenizer, device):
  """
  对单条中文句子进行翻译。

  :param zh_sentence: 中文句子。
  :param encoder: 编码器。
  :param decoder: 解码器。
  :param zh_tokenizer: 中文分词器。
  :param en_tokenizer: 英文分词器。
  :param device: 设备。
  :return: 英文翻译句子。
  """
  input_ids = zh_tokenizer.encode(zh_sentence, seq_len=config.SEQ_LEN, add_sos_eos=False)
  input_tensor = torch.tensor([input_ids], device=device)

  generated = predict_batch(input_tensor, encoder, decoder, en_tokenizer, device)
  en_indexes = generated[0]
  en_sentence = en_tokenizer.decode(en_indexes)

  return en_sentence

def run_predict():
  """
  启动交互式翻译程序。
  """
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  zh_tokenizer = ChineseTokenizer.from_vocab(config.PROCESSED_DATA_DIR / 'zh_vocab.txt')
  en_tokenizer = EnglishTokenizer.from_vocab(config.PROCESSED_DATA_DIR / 'en_vocab.txt')

  encoder = TranslationEncoder(
    vocab_size=zh_tokenizer.vocab_size,
    padding_index=zh_tokenizer.pad_token_index
  ).to(device)
  encoder.load_state_dict(torch.load(config.MODELS_DIR / 'encoder.pt'))

  decoder = TranslationDecoder(
    vocab_size=en_tokenizer.vocab_size,
    padding_index=en_tokenizer.pad_token_index
  ).to(device)
  decoder.load_state_dict(torch.load(config.MODELS_DIR / 'decoder.pt'))

  print('欢迎使用翻译系统，请输入中文句子：（输入 q 或 quit 退出）')
  while True:
    user_input = input('中文：')
    if user_input in ['q', 'quit']:
      print('谢谢使用，再见！')
      break
    if not user_input:
      print('请输入内容')
      continue

    result = predict(user_input, encoder, decoder, zh_tokenizer, en_tokenizer, device)
    print(f'英文：{result}')

if __name__ == '__main__':
  run_predict()
